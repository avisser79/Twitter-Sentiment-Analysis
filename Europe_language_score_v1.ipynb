{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP74i1TlJ0gxDFM0fN7J5TN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avisser79/Twitter-Sentiment-Analysis/blob/main/Europe_language_score_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSC9Lan9OOtk",
        "outputId": "43789878-7d11-4849-d0bf-83cb54d3aaa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "<ipython-input-14-250e58e22c26>:13: DtypeWarning: Columns (6,7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('Europe_sentiment_analysis_3.csv')\n",
            "<ipython-input-14-250e58e22c26>:92: DtypeWarning: Columns (6,7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('Europe_sentiment_analysis_3.csv')\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import textstat\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Read the DataFrame from the CSV file\n",
        "df = pd.read_csv('Europe_sentiment_analysis_3.csv')\n",
        "\n",
        "# Fill NaN values in the likes_count column with a default value of -1\n",
        "df['likes_count'].fillna(-1, inplace=True)\n",
        "\n",
        "# Convert the likes_count column to float\n",
        "df['likes_count'] = pd.to_numeric(df['likes_count'], errors='coerce')\n",
        "\n",
        "# Fill NaN values in the reply_count column with a default value of -1\n",
        "df['reply_count'].fillna(-1, inplace=True)\n",
        "\n",
        "# Convert the reply_count column to float\n",
        "df['reply_count'] = pd.to_numeric(df['reply_count'], errors='coerce')\n",
        "\n",
        "# Fill NaN values in the quote_count column with a default value of '-1'\n",
        "df['quote_count'].fillna('-1', inplace=True)\n",
        "\n",
        "# Convert the quote_count column to object (string)\n",
        "df['quote_count'] = df['quote_count'].astype(str)\n",
        "\n",
        "# Fill NaN values in the retweet_count column with a default value of -1\n",
        "df['retweet_count'].fillna(-1, inplace=True)\n",
        "\n",
        "# Convert the retweet_count column to integer\n",
        "df['retweet_count'] = df['retweet_count'].astype(str)\n",
        "\n",
        "# Function to calculate cohesion score\n",
        "def calculate_cohesion(tweet):\n",
        "    if isinstance(tweet, str):\n",
        "        # Tokenize the tweet into individual words\n",
        "        tokens = word_tokenize(tweet.lower())\n",
        "\n",
        "        # Remove punctuation marks\n",
        "        tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "        # Calculate the frequency distribution of tokens\n",
        "        freq_dist = FreqDist(tokens)\n",
        "\n",
        "        # Calculate the cohesion score based on the number of unique words\n",
        "        cohesion_score = len(freq_dist) / len(tokens)\n",
        "\n",
        "        return cohesion_score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to calculate coherence score\n",
        "def calculate_coherence(tweet, keywords):\n",
        "    if isinstance(tweet, str):\n",
        "        # Tokenize the tweet into individual words\n",
        "        tokens = word_tokenize(tweet.lower())\n",
        "\n",
        "        # Remove punctuation marks\n",
        "        tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "        # Calculate the frequency distribution of tokens\n",
        "        freq_dist = FreqDist(tokens)\n",
        "\n",
        "        # Calculate the coherence score based on the presence of keywords\n",
        "        coherence_score = sum(freq_dist[keyword] for keyword in keywords) / len(tokens)\n",
        "\n",
        "        return coherence_score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to fetch keywords from a tweet\n",
        "def fetch_keywords(tweet):\n",
        "    if isinstance(tweet, float):\n",
        "        return []  # Return an empty list for float values\n",
        "    else:\n",
        "        # Perform part-of-speech tagging on the tweet\n",
        "        blob = TextBlob(tweet)\n",
        "        pos_tags = blob.tags\n",
        "\n",
        "        # Extract nouns and adjectives as keywords\n",
        "        keywords = [word for word, pos in pos_tags if pos.startswith(\"N\") or pos.startswith(\"J\")]\n",
        "\n",
        "        return keywords\n",
        "\n",
        "# Read the DataFrame from the CSV file\n",
        "df = pd.read_csv('Europe_sentiment_analysis_3.csv')\n",
        "\n",
        "# Calculate cohesion and coherence for each tweet in the dataframe\n",
        "df['cohesion'] = df['cleaned_tweet'].apply(calculate_cohesion)\n",
        "df['keywords'] = df['cleaned_tweet'].apply(fetch_keywords)\n",
        "df['coherence_score'] = df.apply(lambda x: calculate_coherence(x['cleaned_tweet'], x['keywords']), axis=1)\n",
        "\n",
        "def calculate_flesch_kincaid(tweet):\n",
        "    try:\n",
        "        return textstat.flesch_kincaid_grade(tweet)\n",
        "    except TypeError:\n",
        "        return 0\n",
        "\n",
        "def calculate_gunning_fog(tweet):\n",
        "    if isinstance(tweet, str):\n",
        "        return textstat.gunning_fog(tweet)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Calculate Flesch-Kincaid Grade Level\n",
        "df['Flesch_Kincaid'] = df['cleaned_tweet'].apply(calculate_flesch_kincaid)\n",
        "\n",
        "# Calculate Gunning_Fog\n",
        "df['Gunning_Fog'] = df['cleaned_tweet'].apply(calculate_gunning_fog)\n",
        "\n",
        "# Calculate the z-scores for the relevant columns\n",
        "columns_to_normalize = ['cohesion', 'coherence_score', 'Flesch_Kincaid', 'Gunning_Fog']\n",
        "for column in columns_to_normalize:\n",
        "    df[f'z_{column}'] = (df[column] - df[column].mean()) / df[column].std()\n",
        "\n",
        "# Calculate the language score based on the weights\n",
        "df['language_score'] = 0.4 * df['z_coherence_score'] - 0.3 * df['z_Flesch_Kincaid'] + 0.2 * df['z_cohesion'] - 0.1 * df['z_Gunning_Fog']\n",
        "\n",
        "df.to_csv('Europe_sentiment_analysis_with_language_score.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCau3lxdQPx1",
        "outputId": "c6f30f30-a97d-4991-874f-a0a24e951bbe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QZ1uWukQSFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}